{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluating Topic Distances of Authors in Twitter-Conversations\n",
    "\n",
    "From the project plan this adresses the following tasks:\n",
    "\n",
    "4. Use BERTopic to analyze the topics\n",
    "   - [ ] encode the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "   id      conversation_id           created_at  \\\n0   1  1435703745304612870  2021-09-08 20:37:01   \n1   2  1435664738353098756  2021-09-08 18:02:01   \n2   3  1435663883595706370  2021-09-08 17:58:37   \n\n                                                text  author_id  \\\n0  RT @OregonOEM: üö©üå©üî• Red Flag and #FireWeather w...   14838508   \n1  It's #NationalPreparednessMonth. Help ensure t...   14838508   \n2  RT @OregonOEM: Oregon is still recovering from...   14838508   \n\n   in_reply_to_user_id  \n0                  NaN  \n1                  NaN  \n2                  NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>conversation_id</th>\n      <th>created_at</th>\n      <th>text</th>\n      <th>author_id</th>\n      <th>in_reply_to_user_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1435703745304612870</td>\n      <td>2021-09-08 20:37:01</td>\n      <td>RT @OregonOEM: üö©üå©üî• Red Flag and #FireWeather w...</td>\n      <td>14838508</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1435664738353098756</td>\n      <td>2021-09-08 18:02:01</td>\n      <td>It's #NationalPreparednessMonth. Help ensure t...</td>\n      <td>14838508</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1435663883595706370</td>\n      <td>2021-09-08 17:58:37</td>\n      <td>RT @OregonOEM: Oregon is still recovering from...</td>\n      <td>14838508</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Create your connection.\n",
    "from twitter.nlp_util import process_tweet\n",
    "\n",
    "cnx = sqlite3.connect('db.sqlite3')\n",
    "\n",
    "df = pd.read_sql_query(\n",
    "    \"SELECT id, conversation_id, created_at, text, author_id,in_reply_to_user_id FROM delab_timeline WHERE lang='en'\",\n",
    "    cnx)\n",
    "df.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "author_id\n16558158               478\n18616003               946\n26998226               469\n382814447              446\n1005470991668084736    445\n1106611172462219265    491\n1162371171805011968    447\n1239172010363826183    427\n1292908140975943681    414\n1402252385427222528    414\n1403930956428460035    441\ndtype: int64"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reduced = df[[\"author_id\", \"text\", \"id\"]]\n",
    "#df_reduced = df_reduced.groupby('author_id')\n",
    "# df_reduced.count()\n",
    "\n",
    "df_reshaped = df_reduced.pivot(index=\"id\", columns=\"author_id\", values=\"text\")\n",
    "mask = 400 > df_reshaped.nunique()\n",
    "mask = mask[mask == True]\n",
    "df_reshaped.drop(columns=mask.index, inplace=True)\n",
    "df_reshaped.nunique()  # the number of tweets of the authors that have more then 400 tweets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following takes the pandas dataframe and converts it to a dictionary with the author ids as keys and the twitter\n",
    "corpora as values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "id\n8247                                   @ncreen_same ouch!\n8248    That last tweet got some responses from spambo...\n8249    'virtually' virtually means: real\\n\\nSo it sho...\n8250                               Sounds great on paper!\n8251    Any .com.au registrar recommendations? So far ...\n                              ...                        \n8742    RT @paydirtapp: Check out our Free Invoice Cre...\n8743    @taitems @mmilo yeah nice site, suggestion: ht...\n8744    RT @MichaelFHansen: Zendesk eyes Southeast Asi...\n8745    While LinkedIn has been changing drastically o...\n8746    Another LinkedIn email fail‚Ä¶ now they want me ...\nName: 16558158, Length: 478, dtype: object"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_reshaped.shape\n",
    "author_corpora_cleaned = {}\n",
    "author_corpora = df_reshaped.to_dict(orient=\"series\")\n",
    "for author_id, tweets in author_corpora.items():\n",
    "    author_corpora_cleaned[author_id] = tweets.dropna()\n",
    "\n",
    "example_corpus = author_corpora_cleaned[next(iter(author_corpora))]\n",
    "example_corpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using sentence transformers from:\n",
    "\n",
    "```latex\n",
    "    @misc{grootendorst2020bertopic,\n",
    "      author       = {Maarten Grootendorst},\n",
    "      title        = {BERTopic: Leveraging BERT and c-TF-IDF to create easily interpretable topics.},\n",
    "      year         = 2020,\n",
    "      publisher    = {Zenodo},\n",
    "      version      = {v0.7.0},\n",
    "      doi          = {10.5281/zenodo.4381785},\n",
    "      url          = {https://doi.org/10.5281/zenodo.4381785}\n",
    "    }\n",
    "\n",
    "    @inproceedings{reimers-2019-sentence-bert,\n",
    "        title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n",
    "        author = \"Reimers, Nils and Gurevych, Iryna\",\n",
    "        booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n",
    "        month = \"11\",\n",
    "        year = \"2019\",\n",
    "        publisher = \"Association for Computational Linguistics\",\n",
    "        url = \"https://arxiv.org/abs/1908.10084\",\n",
    "    }\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-17 14:56:17.405806: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/15 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2da07297edb14e0b8c9a92b3d18cc331"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-17 14:56:25,783 - BERTopic - Transformed documents to Embeddings\n",
      "2021-09-17 14:56:34,542 - BERTopic - Reduced dimensionality with UMAP\n",
      "2021-09-17 14:56:34,570 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n"
     ]
    },
    {
     "data": {
      "text/plain": "   Topic  Count                                  Name\n0      0    141           0_ncreen_same_http_mmilo_it\n1     -1    118                      -1_on_just_co_it\n2      1     55  1_melbourne_co_7pmanywhere_headstart\n3      2     52  2_support_crazydomains_bugherd_issue\n4      3     46         3_bugherd_zendesk_http_zapier\n5      4     33         4_paydirtapp_https_http_money\n6      5     19  5_drinking_brewsmithau_beer_heineken\n7      6     14               6_web_job_software_hire",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Topic</th>\n      <th>Count</th>\n      <th>Name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>141</td>\n      <td>0_ncreen_same_http_mmilo_it</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1</td>\n      <td>118</td>\n      <td>-1_on_just_co_it</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>55</td>\n      <td>1_melbourne_co_7pmanywhere_headstart</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>52</td>\n      <td>2_support_crazydomains_bugherd_issue</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>46</td>\n      <td>3_bugherd_zendesk_http_zapier</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>4</td>\n      <td>33</td>\n      <td>4_paydirtapp_https_http_money</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>5</td>\n      <td>19</td>\n      <td>5_drinking_brewsmithau_beer_heineken</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>6</td>\n      <td>14</td>\n      <td>6_web_job_software_hire</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentences = list(example_corpus)\n",
    "#sentences = [\"This is an example sentence with Trump and Merkel as NER\", \"Each sentence is converted and it is a great Civil War\"]\n",
    "\n",
    "#model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "#embeddings = model.encode(sentences)\n",
    "#print(embeddings)\n",
    "\n",
    "topic_model = BERTopic(embedding_model=\"sentence-transformers/all-mpnet-base-v2\", verbose=True)\n",
    "topics, probs = topic_model.fit_transform(sentences)\n",
    "topic_model.get_topic_info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**This looks promising as it includes the verbs in the topic description as opposed to the NER approach!**\n",
    "- It requires less data as it can use the full sentence (based on the transformer model)\n",
    "- We now need to fit the BERT-Model to all the tweets we have available given a language\n",
    "- [ ] collect all the tweets from the conversations\n",
    "- [X] collect all the tweets from the authors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "author_tweets_texts = list(df.text)\n",
    "print(\"the author tweets are: {}\".format(len(author_tweets_texts)))\n",
    "print(author_tweets_texts[0:5])\n",
    "\n",
    "df_conversations = pd.read_sql_query(\n",
    "    \"SELECT id, text, author_id FROM delab_tweet\",\n",
    "    cnx)\n",
    "#  WHERE lang='en' should be in there but the field is missing TODO!\n",
    "\n",
    "conversation_tweets_texts = list(df_conversations.text)\n",
    "print(\"the conversation tweets are: {}\".format(len(conversation_tweets_texts)))\n",
    "print(conversation_tweets_texts[0:5])\n",
    "\n",
    "\n",
    "corpus_for_fitting = author_tweets_texts + conversation_tweets_texts\n",
    "# corpus_for_fitting = author_tweets_texts\n",
    "corpus_for_fitting_sentences = []\n",
    "for tweet in corpus_for_fitting:\n",
    "    for sentence in tweet.split(\".\"):\n",
    "        #clean_tweet = process_tweet(sentence)\n",
    "        #clean_tweet_string = ' '.join(clean_tweet)\n",
    "        #corpus_for_fitting_sentences.append(clean_tweet_string)\n",
    "        corpus_for_fitting_sentences.append(sentence)\n",
    "print(\"corpus for fitting is: {}\".format(len(corpus_for_fitting_sentences)))\n",
    "\n",
    "topic_model_2 = BERTopic(embedding_model=\"sentence-transformers/all-mpnet-base-v2\", verbose=True)\n",
    "topics, probs = topic_model_2.fit_transform(corpus_for_fitting_sentences)\n",
    "topic_model_2.get_topic_info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the author tweets are: 7362\n",
      "['RT @OregonOEM: üö©üå©üî• Red Flag and #FireWeather warnings are in the forecast for southern, central and eastern parts of the state: https://t.c‚Ä¶', \"It's #NationalPreparednessMonth. Help ensure there's a steady supply of blood on hand to meet the needs of patients every day &amp; be prepared for emergencies of all types. Schedule a blood or platelet donation ‚Äì there's a critical need right now: https://t.co/MOUHKbBeFU https://t.co/RkeEa11OCv\", \"RT @OregonOEM: Oregon is still recovering from the impacts of the 2020 wildfires. That's why OEM urges Oregonians to honor survivors by tak‚Ä¶\", 'RT @Readygov: This National Preparedness Month, we are reminding you to build your emergency kit! \\n\\nEach kit should contain items to meet y‚Ä¶', \"RT @OregonOEM: Today, we remember the September 2020 wildfires and the impacts they've had across our state. We are grateful for all the su‚Ä¶\"]\n",
      "the conversation tweets are: 400\n",
      "[\"‚òÄÔ∏è ‚òÄÔ∏è We've said it a lot this summer, but if your home gets too hot, libraries, malls, &amp; theaters are great places to escape too. Also call 211 or visit @211info and your local county for additional cooling center resources. ü•µ https://t.co/bdaaCCW942\", \"@RedCrossCasc @OregonGovBrown @211info Biggest scandal in world and Oregonian history. They have injected billions with a NON approved never before used experimental mRNA shot, telling all it was safe and would protect them, and they LIED. Now they are blaming 'delta'with NO proof, lying again. It is the shots people!\", '@RedCrossCasc @OregonGovBrown @211info You are being lied to. There is no \"delta\". Proof here...ask yourselves and the governor, for them to again take idiotic non-science masking to us, where is the REAL PROOF of this? Show us the isolated \"delta\" virus now! https://t.co/IpRD84rYZr', '@RedCrossCasc @OregonGovBrown @211info But make sure you wear your mask ü•¥ü•¥ü§°üêëü•¥ü•¥ü•¥ #commiebastards https://t.co/yBazZXOsq0', \"@RedCrossCasc @OregonGovBrown @211info You see people, we don't get hospitals full of respiratory virus patients in the middle of summer!..unless they have been injected with them. Most are vaccinated, but they will try to cover this up, guarantee. This is massive criminality/https://t.co/WfXukXdroG\"]\n",
      "corpus for fitting is: 18785\n"
     ]
    },
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/588 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a718282b0a4455eb93edda373b900e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-17 15:31:03,539 - BERTopic - Transformed documents to Embeddings\n",
      "2021-09-17 15:31:11,402 - BERTopic - Reduced dimensionality with UMAP\n",
      "2021-09-17 15:31:13,048 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n"
     ]
    },
    {
     "data": {
      "text/plain": "     Topic  Count                                           Name\n0       -1   6025                          -1_she_know_trump_her\n1        0   4235                0_fatal_fascists_fathom_fathoms\n2        1   1336           1_zxxmo0yfrh_gc0xaznhpq_3ds_emulator\n3        2    318                    2_watch_thread_kip_boldness\n4        3    293                3_covid_vaccine_mrna_vaccinated\n..     ...    ...                                            ...\n200    203     10     203_abc_lie_dailyexposeteam_crocodilekatie\n199    204     10  204_protection_protect_participation_fighters\n198    205     10               205_puke_acid_swallow_quantities\n196    201     10         201_acosta_hardball_editorialize_jimbo\n208    207     10         207_drama_theatre_dramatic_protagonist\n\n[209 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Topic</th>\n      <th>Count</th>\n      <th>Name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1</td>\n      <td>6025</td>\n      <td>-1_she_know_trump_her</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>4235</td>\n      <td>0_fatal_fascists_fathom_fathoms</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1336</td>\n      <td>1_zxxmo0yfrh_gc0xaznhpq_3ds_emulator</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>318</td>\n      <td>2_watch_thread_kip_boldness</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>293</td>\n      <td>3_covid_vaccine_mrna_vaccinated</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>203</td>\n      <td>10</td>\n      <td>203_abc_lie_dailyexposeteam_crocodilekatie</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>204</td>\n      <td>10</td>\n      <td>204_protection_protect_participation_fighters</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>205</td>\n      <td>10</td>\n      <td>205_puke_acid_swallow_quantities</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>201</td>\n      <td>10</td>\n      <td>201_acosta_hardball_editorialize_jimbo</td>\n    </tr>\n    <tr>\n      <th>208</th>\n      <td>207</td>\n      <td>10</td>\n      <td>207_drama_theatre_dramatic_protagonist</td>\n    </tr>\n  </tbody>\n</table>\n<p>209 rows √ó 3 columns</p>\n</div>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "[('covid', 0.024164840806527484),\n ('vaccine', 0.017095397907528627),\n ('mrna', 0.0111956963463027),\n ('vaccinated', 0.009954721382282957),\n ('vaccines', 0.009954721382282957),\n ('pandemic', 0.009559790766417903),\n ('coronavirus', 0.008164559412939898),\n ('vaxxx', 0.0075630464177102955),\n ('inoculation', 0.006482611215180254),\n ('vax', 0.006291761201900802)]"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model_2.get_topic(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After having fit the model to the language we have availabel in the db,\n",
    "we can now predict the topics of the authors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example corpus 2 is:\n",
      " ['RT @aevanko: When you‚Äôve played too much Soul Sacrifice, everything else in your life suffers. https://t.co/t0nt6lZPKj', 'RT @equalityAlec: Thread.  Have you ever heard of \"civil asset forfeiture\"? You\\'re never going to think about the police the same way again‚Ä¶', 'RT @equalityAlec: If all the crimes committed by police and jail/prison guards was counted, it would completely change the police crime sta‚Ä¶', 'RT @equalityAlec: A few thoughts about \"crime.\"  The concept of ‚Äúcrime‚Äù is created and manipulated by people who have power. Throughout U.S‚Ä¶', 'RT @equalityAlec: UPDATED THREAD. You\\'re going to hear a lot about how cops need more resources because \"crime is surging\" in the next few‚Ä¶']\n"
     ]
    },
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/14 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9dc97c7638b6496880a05f68cee34bae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "53.0     24\n8.0      22\n3.0      20\n130.0    13\n47.0     12\n113.0     9\n110.0     8\n159.0     7\n19.0      6\n11.0      5\n14.0      5\n134.0     4\n9.0       4\n30.0      4\n140.0     3\n42.0      3\n20.0      3\n6.0       3\n16.0      2\n89.0      2\n66.0      2\n69.0      2\n126.0     2\n75.0      2\n80.0      2\n129.0     1\n205.0     1\n131.0     1\n152.0     1\n173.0     1\n165.0     1\n145.0     1\n147.0     1\n160.0     1\n0.0       1\n74.0      1\n116.0     1\n111.0     1\n104.0     1\n96.0      1\n82.0      1\n71.0      1\n70.0      1\n59.0      1\n37.0      1\n33.0      1\n27.0      1\n18.0      1\n7.0       1\n4.0       1\n206.0     1\ndtype: int64"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# topics2, probs2 = topic_model.fit_transform(example_corpus_2)\n",
    "# topic_info = topic_model.get_topic_info()\n",
    "# print(topic_info)\n",
    "\n",
    "author_ids = author_corpora_cleaned.keys()\n",
    "author_ids_list = []\n",
    "for key in author_ids:\n",
    "    author_ids_list.append(key)\n",
    "\n",
    "#example_corpus_1 = list(author_corpora_cleaned[author_ids_list[3]])\n",
    "#print(\"example corpus 1 is:\\n {}\\n\".format(example_corpus_1[0:5]))\n",
    "#suggested_topics = topic_model_2.transform(example_corpus_1)\n",
    "#np_suggested_topics = np.array(suggested_topics[0])\n",
    "#index_of_suggested_topic = np_suggested_topics.argmax()\n",
    "#print (\"suggested_topic for corpus 1 is {}\".format(topic_model_2.get_topic(index_of_suggested_topic)))\n",
    "\n",
    "\n",
    "example_corpus_2 = list(author_corpora_cleaned[author_ids_list[4]])\n",
    "print(\"example corpus 2 is:\\n {}\".format(example_corpus_2[0:5]))\n",
    "suggested_topics2 = topic_model_2.transform(example_corpus_2)\n",
    "np_suggested_topics2 = np.array(suggested_topics2[0])\n",
    "pd_suggested_topics2 = pd.DataFrame(np_suggested_topics2, columns={0:\"topic_counts\"})\n",
    "value_counts2 = pd_suggested_topics2[pd_suggested_topics2>=0].value_counts()\n",
    "value_counts2[0]\n",
    "#value_counts2.shape\n",
    "#index_of_suggested_topic2 = np_suggested_topics2.argmax()\n",
    "#print(index_of_suggested_topic2)\n",
    "#print (\"suggested_topic for corpus 2 is {}\".format(topic_model_2.get_topic(index_of_suggested_topic2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}